---
description: Orchestrate complex tasks using RVGB architecture with PostgreSQL memory, Redis Blackboard, and Docker verification
argument-hint: [task description]
---

# ORCHESTRATION MODE v3.0 (RVGB - Recursive Verifying Graph-Blackboard)

**Task:** $ARGUMENTS

---

## RVGB ARCHITECTURE OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RVGB: Recursive Verifying Graph-Blackboard                â”‚
â”‚                                                             â”‚
â”‚  PostgreSQL â†’ Hybrid tiered memory (episodic + semantic)   â”‚
â”‚  Redis â†’ Blackboard for agent coordination                 â”‚
â”‚  Docker â†’ Shadow workspace for safe verification           â”‚
â”‚  MCP â†’ Memory server for tool-based access                 â”‚
â”‚  Knowledge Graph â†’ Temporal entity relationships           â”‚
â”‚  Reflexion â†’ Stored reflections + retrieval                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## MANDATORY BEHAVIOR - NO EXCEPTIONS

### RULE 1: NEVER WRITE CODE
- You MUST NOT write any implementation code
- ALL code comes from specialist agents via Task tool
- Violation of this rule = FAILURE

### RULE 2: ALWAYS USE TASK TOOL
For ANY implementation, you MUST call:
```
Task(subagent_type="general-purpose", prompt="You are @[specialist]. [detailed task]...")
```

### RULE 3: ALWAYS UPDATE MEMORY
After completion, you MUST:
- Store episodic memory in PostgreSQL
- Update knowledge graph entities/relations
- Run memory consolidation on completion
- Store reflections if failures occurred

### RULE 4: USE MODEL ROUTING
Route tasks to appropriate model based on complexity:
- **opus** â†’ Architecture, complex planning, critical decisions
- **sonnet** â†’ Implementation, standard coding tasks
- **haiku** â†’ Simple fixes, formatting, boilerplate

---

## EXECUTION FLOW (15-STEP RVGB PIPELINE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Initialize Infrastructure                          â”‚
â”‚  STEP 2: Load Tiered Memory (PostgreSQL)                    â”‚
â”‚  STEP 3: Retrieve Past Reflections                          â”‚
â”‚  STEP 4: Context Compression (AST Repo Map)                 â”‚
â”‚  STEP 5: Analyze & Plan (with Model Routing)                â”‚
â”‚  STEP 6: Write Plan to Blackboard                           â”‚
â”‚  STEP 7: Create Shadow Workspace (Docker)                   â”‚
â”‚  STEP 8: Delegate to Agents (read/write Blackboard)         â”‚
â”‚  STEP 9: Verification Gauntlet (in Shadow)                  â”‚
â”‚  STEP 10: TDD Loop (3-strike max)                           â”‚
â”‚  STEP 11: Multi-Perspective Review                          â”‚
â”‚  STEP 12: Reflexion - Generate & Store                      â”‚
â”‚  STEP 13: Commit Shadow or Rollback                         â”‚
â”‚  STEP 14: Memory Consolidation ("Sleep")                    â”‚
â”‚  STEP 15: Report to User                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## STEP 1: Initialize Infrastructure

**Goal:** Ensure all RVGB components are running and accessible

### 1.1 Check Environment Variables
```bash
# Required environment variables:
# - DATABASE_URL: PostgreSQL connection string
# - REDIS_URL: Redis connection string (default: redis://localhost:6379)
# - DOCKER_HOST: Docker daemon (default: unix:///var/run/docker.sock)
# - MCP_MEMORY_SERVER_PORT: MCP server port (default: 3000)

# Validate each is set and accessible
echo "Checking DATABASE_URL..."
echo "Checking REDIS_URL..."
echo "Checking Docker daemon..."
echo "Checking MCP Memory Server..."
```

### 1.2 PostgreSQL Connection
```bash
# Test PostgreSQL connection
psql $DATABASE_URL -c "SELECT 1;" 2>&1

# Verify required tables exist:
# - episodic_memory (raw episodes from orchestration runs)
# - semantic_memory (consolidated patterns and knowledge)
# - procedural_memory (reflections and learned behaviors)
# - entities (knowledge graph nodes)
# - relations (knowledge graph edges)

# If tables missing, create them (run schema migration)
```

**PostgreSQL Schema:**
```sql
-- Episodic Memory (raw experiences)
CREATE TABLE IF NOT EXISTS episodic_memory (
    id SERIAL PRIMARY KEY,
    pipeline_run_id VARCHAR(64) NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    event_type VARCHAR(32) NOT NULL, -- 'task_start', 'agent_call', 'verification', etc.
    agent_name VARCHAR(64),
    task_description TEXT,
    context JSONB, -- Full context snapshot
    result JSONB, -- Outcome, errors, metrics
    importance FLOAT DEFAULT 0.5, -- 0.0-1.0
    last_accessed TIMESTAMPTZ DEFAULT NOW()
);

-- Semantic Memory (consolidated patterns)
CREATE TABLE IF NOT EXISTS semantic_memory (
    id SERIAL PRIMARY KEY,
    pattern_name VARCHAR(128) UNIQUE NOT NULL,
    category VARCHAR(64) NOT NULL,
    description TEXT,
    key_elements JSONB,
    success_rate FLOAT DEFAULT 1.0,
    times_used INTEGER DEFAULT 1,
    last_used TIMESTAMPTZ DEFAULT NOW(),
    utility_score FLOAT GENERATED ALWAYS AS (
        (times_used * 0.4) + (success_rate * 0.3) +
        (EXTRACT(EPOCH FROM (NOW() - last_used)) / 3600 * 0.3)
    ) STORED
);

-- Procedural Memory (reflections)
CREATE TABLE IF NOT EXISTS procedural_memory (
    id SERIAL PRIMARY KEY,
    reflection_text TEXT NOT NULL,
    failure_context JSONB, -- What failed, why, how fixed
    created_at TIMESTAMPTZ DEFAULT NOW(),
    relevance_tags TEXT[], -- For retrieval
    applied_count INTEGER DEFAULT 0,
    last_applied TIMESTAMPTZ
);

-- Knowledge Graph: Entities
CREATE TABLE IF NOT EXISTS entities (
    id SERIAL PRIMARY KEY,
    entity_type VARCHAR(64) NOT NULL, -- 'file', 'function', 'module', 'component'
    entity_name VARCHAR(255) NOT NULL,
    metadata JSONB,
    first_seen TIMESTAMPTZ DEFAULT NOW(),
    last_modified TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(entity_type, entity_name)
);

-- Knowledge Graph: Relations
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    source_entity_id INTEGER REFERENCES entities(id) ON DELETE CASCADE,
    relation_type VARCHAR(64) NOT NULL, -- 'imports', 'calls', 'uses', 'depends_on'
    target_entity_id INTEGER REFERENCES entities(id) ON DELETE CASCADE,
    weight FLOAT DEFAULT 1.0, -- Importance of relationship
    first_seen TIMESTAMPTZ DEFAULT NOW(),
    last_seen TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(source_entity_id, relation_type, target_entity_id)
);

-- Indexes for fast retrieval
CREATE INDEX IF NOT EXISTS idx_episodic_pipeline ON episodic_memory(pipeline_run_id);
CREATE INDEX IF NOT EXISTS idx_episodic_timestamp ON episodic_memory(timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_episodic_importance ON episodic_memory(importance DESC);
CREATE INDEX IF NOT EXISTS idx_semantic_category ON semantic_memory(category);
CREATE INDEX IF NOT EXISTS idx_semantic_utility ON semantic_memory(utility_score DESC);
CREATE INDEX IF NOT EXISTS idx_procedural_tags ON procedural_memory USING GIN(relevance_tags);
CREATE INDEX IF NOT EXISTS idx_entities_type_name ON entities(entity_type, entity_name);
CREATE INDEX IF NOT EXISTS idx_relations_source ON relations(source_entity_id);
CREATE INDEX IF NOT EXISTS idx_relations_target ON relations(target_entity_id);
```

### 1.3 Redis Blackboard Connection
```bash
# Test Redis connection
redis-cli -u $REDIS_URL PING 2>&1

# Initialize Blackboard structures:
# - artifacts:[pipeline_id]:* (HASH) - Stores artifacts (plan, code, tests)
# - locks:[pipeline_id]:* (STRING with expiry) - File-level locks
# - status:[pipeline_id] (HASH) - Pipeline status tracking
# - agents:[pipeline_id]:* (LIST) - Agent activity log
```

**Redis Blackboard Schema:**
```python
# Artifact structure (stored as JSON in Redis Hash)
{
    "artifact_id": "plan_v1",
    "artifact_type": "PLAN|CODE|TEST|REVIEW|REFLECTION",
    "content": "...",
    "created_by": "orchestrator|agent_name",
    "created_at": "2025-11-29T10:30:00Z",
    "version": 1,
    "dependencies": ["artifact_id_1", "artifact_id_2"]
}

# Lock structure (Redis String with TTL)
Key: locks:[pipeline_id]:src/main.py
Value: agent_name
TTL: 300 seconds (auto-release)

# Status tracking (Redis Hash)
Key: status:[pipeline_id]
Fields:
  - phase: "planning|implementation|verification|review|complete"
  - progress: "0-100"
  - active_agents: "agent1,agent2"
  - errors: "0"
```

### 1.4 Docker Shadow Workspace
```bash
# Check if Docker is running
docker info 2>&1

# Build base shadow workspace image (if not exists)
# This is a lightweight container with:
# - Git
# - Common build tools (cargo, npm, pip, etc.)
# - Testing frameworks
# - Linters/formatters

# Check for existing image
if ! docker images | grep -q "rvgb-shadow-workspace"; then
    echo "Building shadow workspace image..."
    # Build from Dockerfile (if available) or use alpine + tools
fi
```

**Shadow Workspace Dockerfile:**
```dockerfile
FROM alpine:3.19

# Install base tools
RUN apk add --no-cache \
    git \
    bash \
    curl \
    python3 py3-pip \
    nodejs npm \
    rust cargo \
    go \
    openjdk11 \
    && rm -rf /var/cache/apk/*

# Install testing frameworks
RUN pip3 install pytest pytest-cov mypy ruff bandit --break-system-packages
RUN npm install -g jest @typescript-eslint/parser eslint
RUN cargo install cargo-test

# Set working directory
WORKDIR /workspace

# Entry point: run verification gauntlet
COPY verify.sh /usr/local/bin/verify
RUN chmod +x /usr/local/bin/verify

CMD ["/bin/bash"]
```

### 1.5 MCP Memory Server
```bash
# Check if MCP Memory Server is running
curl -s http://localhost:${MCP_MEMORY_SERVER_PORT:-3000}/health 2>&1

# If not running, start it (or skip if not installed)
# MCP Memory Server provides tool-based access to PostgreSQL memory
```

**MCP Memory Tools Available:**
```
- search_memory(query, limit=5) â†’ Search episodic + semantic memory
- get_similar_patterns(context) â†’ Retrieve patterns by similarity
- store_episode(event_type, context, result) â†’ Store new episode
- get_relevant_reflections(context) â†’ Retrieve past reflections
- update_knowledge_graph(entity, relation, target) â†’ Update graph
- consolidate_memory(pipeline_run_id) â†’ Trigger consolidation
```

### 1.6 Infrastructure Health Report
```markdown
## Infrastructure Status
- âœ…/âŒ PostgreSQL: [connected/disconnected] (tables: X)
- âœ…/âŒ Redis Blackboard: [connected/disconnected]
- âœ…/âŒ Docker Daemon: [running/stopped]
- âœ…/âŒ Shadow Workspace Image: [built/missing]
- âœ…/âš ï¸/âŒ MCP Memory Server: [running/not installed/error]

**Fallback Mode:**
If any component unavailable, fall back to:
- PostgreSQL â†’ JSON files in /home/rnd/.claude/orchestrator/memory/
- Redis â†’ In-memory dict (no persistence)
- Docker â†’ Direct workspace (no isolation)
- MCP â†’ Direct PostgreSQL queries via psql
```

**SKIP Infrastructure Check if:**
- User says "offline mode" or "no infrastructure"
- Quick mode enabled

---

## STEP 2: Load Tiered Memory (PostgreSQL)

**Goal:** Retrieve relevant past experiences to inform current task

### 2.1 Memory Retrieval with Scoring

**Scoring Formula:**
```
Score = 0.5 * Relevance + 0.3 * Importance + 0.2 * Recency

Where:
- Relevance: Semantic similarity to current task (0.0-1.0)
- Importance: Stored importance value (0.0-1.0)
- Recency = e^(-0.995 * hours_since_access)
```

### 2.2 Episodic Memory Retrieval

**Query episodic_memory table:**
```sql
-- Retrieve top 10 relevant episodes
SELECT
    id,
    event_type,
    agent_name,
    task_description,
    context,
    result,
    importance,
    EXTRACT(EPOCH FROM (NOW() - last_accessed)) / 3600 AS hours_since,
    EXP(-0.995 * EXTRACT(EPOCH FROM (NOW() - last_accessed)) / 3600) AS recency_score
FROM episodic_memory
WHERE
    -- Filter by relevance (simple keyword match for now)
    task_description ILIKE '%[detected_keywords]%'
    OR context::text ILIKE '%[detected_keywords]%'
ORDER BY
    (0.5 * 0.8 + 0.3 * importance + 0.2 * EXP(-0.995 * EXTRACT(EPOCH FROM (NOW() - last_accessed)) / 3600)) DESC
LIMIT 10;

-- Update last_accessed for retrieved episodes
UPDATE episodic_memory
SET last_accessed = NOW()
WHERE id IN ([retrieved_ids]);
```

**Using MCP (if available):**
```
mcp__memory__search_memory(
    query="implement order matching engine with websockets",
    limit=5
)
```

### 2.3 Semantic Memory Retrieval

**Query semantic_memory table:**
```sql
-- Retrieve patterns by category and utility
SELECT
    pattern_name,
    category,
    description,
    key_elements,
    success_rate,
    times_used,
    utility_score
FROM semantic_memory
WHERE
    category = '[detected_category]' -- e.g., 'rust-async', 'frontend-react'
    AND utility_score > 0.3 -- Minimum utility threshold
ORDER BY
    utility_score DESC
LIMIT 5;

-- Update usage stats for retrieved patterns
UPDATE semantic_memory
SET last_used = NOW()
WHERE pattern_name IN ([retrieved_patterns]);
```

**Using MCP (if available):**
```
mcp__memory__get_similar_patterns(
    context="rust websocket order matching"
)
```

### 2.4 Memory Hierarchy

**TIER 1 - Always Load (Core Context):**
- Total orchestrations run
- Common technologies encountered
- Overall success rate
- Recent failure patterns

**TIER 2 - Conditionally Load (Relevant Patterns):**
- Patterns matching detected category
- Episodes with high relevance score (>0.7)
- High-utility patterns (utility_score > 0.8)

**TIER 3 - On-Demand (Archival):**
- Older episodes (>30 days old)
- Low-utility patterns (utility_score < 0.3)
- Edge cases and rare scenarios

### 2.5 Memory Loading Summary

```markdown
## Memory Loaded
**Episodic Memory:**
- Retrieved: 7 episodes (avg score: 0.82)
- Most relevant: "WebSocket connection pooling fix" (score: 0.94)
- Total episodes in DB: 1,247

**Semantic Memory:**
- Patterns loaded: 4
  - rust-async-websockets (utility: 0.91, used: 12x)
  - order-matching-algorithms (utility: 0.87, used: 8x)
  - error-handling-async (utility: 0.76, used: 15x)
  - docker-compose-setup (utility: 0.68, used: 5x)

**Fallback (if PostgreSQL unavailable):**
- Loading from JSON: /home/rnd/.claude/orchestrator/memory/
```

---

## STEP 3: Retrieve Past Reflections

**Goal:** Learn from past failures to avoid repeating mistakes

### 3.1 Reflection Retrieval

**Query procedural_memory table:**
```sql
-- Retrieve reflections by relevance tags
SELECT
    id,
    reflection_text,
    failure_context,
    created_at,
    applied_count
FROM procedural_memory
WHERE
    -- Match by tags (array overlap)
    relevance_tags && ARRAY['[detected_tag_1]', '[detected_tag_2]']::TEXT[]
ORDER BY
    applied_count ASC, -- Prefer less-used reflections (avoid over-fitting)
    created_at DESC     -- Prefer recent learnings
LIMIT 3;

-- Update applied_count for retrieved reflections
UPDATE procedural_memory
SET
    applied_count = applied_count + 1,
    last_applied = NOW()
WHERE id IN ([retrieved_ids]);
```

**Using MCP (if available):**
```
mcp__memory__get_relevant_reflections(
    context="async rust websocket error handling"
)
```

### 3.2 Inject Reflections into Agent Context

**Format reflections for agent prompts:**
```markdown
## PAST LEARNINGS (Reflexion)
These are reflections from similar tasks. Learn from them:

**Reflection 1:** [reflection_text]
- Failure: [failure_context.what_failed]
- Root Cause: [failure_context.root_cause]
- Fix: [failure_context.fix_applied]
- Lesson: [failure_context.lesson]

**Reflection 2:** [...]
```

**Reflection Example:**
```json
{
    "reflection_text": "When implementing async WebSocket handlers in Rust, always use bounded channels (not unbounded) to prevent memory exhaustion under high load.",
    "failure_context": {
        "what_failed": "WebSocket server crashed after 10K concurrent connections",
        "root_cause": "Unbounded mpsc channel accumulated messages faster than processing",
        "fix_applied": "Changed to tokio::sync::mpsc::channel(1000)",
        "lesson": "Always bound async channels with reasonable capacity"
    },
    "relevance_tags": ["rust", "async", "websocket", "memory", "channels"],
    "applied_count": 2,
    "created_at": "2025-11-15T14:20:00Z"
}
```

### 3.3 Reflection Summary

```markdown
## Reflections Retrieved
- **3 reflections** loaded from procedural memory
- Tags matched: rust, async, websocket, error-handling
- Most relevant: "Always bound async channels" (applied 2x)
- Injected into agent prompts for awareness
```

---

## STEP 4: Context Compression (AST Repo Map)

**Goal:** Minimize tokens while maximizing relevant context

### 4.1 Build Repository Map

**Scan project structure:**
```bash
# Find all source files
Glob **/*.{rs,py,ts,tsx,js,jsx,go,java}

# For each file, extract:
# - Function/method signatures (not implementations)
# - Class/struct definitions (not bodies)
# - Import/export statements
# - Type definitions
```

**AST Extraction (example for Rust):**
```rust
// src/engine.rs (actual 500 lines)
// Compressed to:
pub struct OrderEngine { /* 8 fields */ }
impl OrderEngine {
    pub fn new(config: Config) -> Self;
    pub async fn process_order(&mut self, order: Order) -> Result<Fill>;
    pub async fn match_orders(&self) -> Vec<Match>;
    // ... 5 more methods
}
```

### 4.2 PageRank Relevance Scoring

**Build call graph:**
```
Entry points (weight: 1.0):
- src/main.rs â†’ imports src/engine.rs (0.8)
- src/engine.rs â†’ imports src/types.rs (0.6)
- src/types.rs â†’ imports src/utils.rs (0.4)

Centrality scores:
- src/engine.rs: 0.95 (highest, many imports)
- src/types.rs: 0.80 (shared types)
- src/main.rs: 0.75 (entry point)
- src/utils.rs: 0.45 (utility functions)
- tests/*.rs: 0.20 (low priority)
```

### 4.3 Token Budget Management

**Target: 50K tokens**

```
Priority loading:
1. Entry points (main.rs, index.ts) - FULL
2. Files mentioned in task - FULL
3. High-centrality files - SIGNATURES ONLY
4. Low-centrality files - SUMMARY ONLY
5. Tests/configs - SKIP

If exceeds budget:
- Level 1: Load only signatures (no implementations)
- Level 2: Summarize large files
- Level 3: Drop low-relevance files
- Level 4: Request user to narrow scope
```

### 4.4 Context Summary

```markdown
## Repository Map (Compressed)
**Entry Points:**
- src/main.rs (async runtime, 3 modules) - 450 tokens
- src/engine.rs (OrderEngine struct, 8 methods) - 1,200 tokens

**Core Modules:**
- src/types.rs (Order, Fill, Quote types) - 300 tokens
- src/websocket.rs (WebSocket handler, 5 methods) - 800 tokens

**Related:**
- src/utils.rs (summary: logging, time utils) - 50 tokens
- src/config.rs (summary: env loading) - 40 tokens

**Skipped:**
- tests/ (24 files)
- benches/ (3 files)
- examples/ (5 files)

**Token Budget:** 12,450 / 50,000 (24% used)
```

---

## STEP 5: Analyze & Plan (with Model Routing)

**Goal:** Create execution plan with model routing assignments

### 5.1 Task Analysis

**Detect task characteristics:**
```
- Complexity: [low/medium/high]
- Category: [detected from context]
- Technologies: [rust, websockets, async, docker]
- Novelty: [routine/moderate/novel]
- Risk: [low/medium/high]
```

### 5.2 Create Execution Plan

**Generate task breakdown:**
```markdown
## Execution Plan

### Phase 1: Architecture (High Complexity â†’ opus)
- Design WebSocket connection pool architecture
- Define error recovery strategy
- Plan graceful shutdown mechanism

### Phase 2: Implementation (Medium Complexity â†’ sonnet)
- Implement connection pool struct
- Add WebSocket handler with bounded channels
- Implement health check endpoint
- Write error handling for disconnections

### Phase 3: Testing (Standard Complexity â†’ sonnet)
- Write unit tests for connection pool
- Write integration tests for WebSocket flow
- Add stress test for 10K connections

### Phase 4: Refinement (Low Complexity â†’ haiku)
- Add debug logging
- Format code with rustfmt
- Update configuration examples
```

### 5.3 Model Routing Assignments

**Decision Matrix:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TASK                             â”‚  MODEL   â”‚  COST  â”‚ WHY â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚  Architecture design              â”‚  opus    â”‚  $$$$  â”‚ Criticalâ”‚
â”‚  Error recovery strategy          â”‚  opus    â”‚  $$$$  â”‚ Complex â”‚
â”‚  Connection pool implementation   â”‚  sonnet  â”‚  $$    â”‚ Standardâ”‚
â”‚  WebSocket handler                â”‚  sonnet  â”‚  $$    â”‚ Standardâ”‚
â”‚  Unit tests                       â”‚  sonnet  â”‚  $$    â”‚ Coverageâ”‚
â”‚  Integration tests                â”‚  sonnet  â”‚  $$    â”‚ Complex â”‚
â”‚  Stress tests                     â”‚  sonnet  â”‚  $$    â”‚ Non-trivialâ”‚
â”‚  Debug logging                    â”‚  haiku   â”‚  $     â”‚ Trivial â”‚
â”‚  Code formatting                  â”‚  haiku   â”‚  $     â”‚ Automatedâ”‚
â”‚  Config examples                  â”‚  haiku   â”‚  $     â”‚ Boilerplateâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 Cost Estimation

```markdown
## Model Routing Summary
| Model  | Tasks | Estimated Tokens | Est. Cost |
|--------|-------|------------------|-----------|
| opus   | 2     | ~50K out + 10K in | $0.30    |
| sonnet | 5     | ~100K out + 25K in| $0.15    |
| haiku  | 3     | ~15K out + 5K in  | $0.003   |
| **Total** | **10** |              | **$0.45** |
```

---

## STEP 6: Write Plan to Blackboard

**Goal:** Share plan with all agents via Redis Blackboard

### 6.1 Initialize Blackboard for Pipeline

```bash
# Generate unique pipeline ID
PIPELINE_ID="rvgb-$(date +%s)-$(uuidgen | cut -d'-' -f1)"

# Initialize status tracking
redis-cli -u $REDIS_URL HSET status:$PIPELINE_ID \
    phase "planning" \
    progress "0" \
    active_agents "" \
    errors "0" \
    started_at "$(date -Iseconds)"
```

### 6.2 Write Plan Artifact

```python
# Store plan as artifact on Blackboard
plan_artifact = {
    "artifact_id": "plan_v1",
    "artifact_type": "PLAN",
    "content": {
        "phases": [
            {
                "phase_name": "Architecture",
                "tasks": [
                    {
                        "task_id": "task-1",
                        "description": "Design WebSocket connection pool",
                        "assigned_agent": "@rust-architect",
                        "model": "opus",
                        "status": "pending"
                    }
                ]
            },
            # ... more phases
        ]
    },
    "created_by": "orchestrator",
    "created_at": "2025-11-29T10:30:00Z",
    "version": 1,
    "dependencies": []
}

# Write to Redis
redis-cli -u $REDIS_URL HSET artifacts:$PIPELINE_ID:plan_v1 \
    data "$(echo $plan_artifact | jq -c .)"
```

### 6.3 Blackboard Operations

**Write Artifact:**
```bash
blackboard.write_artifact(
    pipeline_id="rvgb-1732880400-a1b2c3",
    artifact_id="plan_v1",
    artifact_type="PLAN",
    content=plan_data,
    created_by="orchestrator"
)
```

**Read Artifact:**
```bash
blackboard.read_artifact(
    pipeline_id="rvgb-1732880400-a1b2c3",
    artifact_id="plan_v1"
)
```

**Acquire Lock (before editing file):**
```bash
blackboard.acquire_lock(
    pipeline_id="rvgb-1732880400-a1b2c3",
    resource="src/engine.rs",
    agent_name="@rust-pro",
    ttl=300  # 5 minutes auto-release
)
```

**Release Lock (after editing):**
```bash
blackboard.release_lock(
    pipeline_id="rvgb-1732880400-a1b2c3",
    resource="src/engine.rs",
    agent_name="@rust-pro"
)
```

### 6.4 Agent Access Pattern

**All agents read plan from Blackboard:**
```
1. Agent spawns â†’ reads plan from Blackboard
2. Agent finds assigned tasks
3. Agent acquires lock on files to modify
4. Agent writes code artifacts to Blackboard
5. Agent releases locks
6. Agent updates task status on Blackboard
```

---

## STEP 7: Create Shadow Workspace (Docker)

**Goal:** Isolated environment for safe verification without affecting main workspace

### 7.1 Launch Shadow Container

```bash
# Create shadow workspace container
SHADOW_ID=$(docker run -d \
    --name "shadow-$PIPELINE_ID" \
    -v "$PWD:/workspace:ro" \
    -w /workspace \
    rvgb-shadow-workspace:latest \
    tail -f /dev/null)

echo "Shadow workspace created: $SHADOW_ID"
```

### 7.2 Copy Project to Shadow

```bash
# Clone project into shadow (isolated copy)
docker exec $SHADOW_ID bash -c "
    git clone /workspace /shadow-workspace
    cd /shadow-workspace
    git config user.name 'RVGB Shadow'
    git config user.email 'shadow@rvgb.local'
"
```

### 7.3 Shadow Workspace Interface

**Python-style API (conceptual):**
```python
with ShadowWorkspace(project_path) as shadow:
    # Apply changes from Blackboard
    shadow.apply_patch(agent_diff)

    # Run verification gauntlet
    results = shadow.verify_all()

    if all(r.passed for r in results):
        # Verification passed â†’ commit to main
        shadow.commit_to_main()
    else:
        # Verification failed â†’ rollback
        shadow.rollback()
        # Feed errors back to agent for retry
```

### 7.4 Shadow Benefits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Complete isolation from main workspace                  â”‚
â”‚  âœ… Safe to run destructive tests                           â”‚
â”‚  âœ… Parallel verification without conflicts                 â”‚
â”‚  âœ… Easy rollback (just delete container)                   â”‚
â”‚  âœ… Reproducible environment (Dockerized)                   â”‚
â”‚  âœ… Can snapshot shadow state at any point                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SKIP Shadow if:**
- Docker not available (fallback to git worktree)
- User says "direct mode"
- Read-only task (no code changes)

---

## STEP 8: Delegate to Agents (read/write Blackboard)

**Goal:** Agents execute tasks, coordinate via Blackboard

### 8.1 Agent Spawning with Context

**Agent receives:**
1. Plan from Blackboard (assigned tasks)
2. Past reflections (STEP 3)
3. Repository map (STEP 4)
4. Blackboard credentials (PIPELINE_ID, REDIS_URL)
5. Shadow workspace path

**Task Call Template:**
```xml
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">opus</parameter>
  <parameter name="prompt">You are @rust-architect, expert in Rust system design.

TASK: Design WebSocket connection pool architecture for high-throughput order matching.

CONTEXT:
- Pipeline ID: rvgb-1732880400-a1b2c3
- Redis Blackboard: $REDIS_URL
- Shadow Workspace: /shadow-workspace (in Docker container: shadow-rvgb-1732880400-a1b2c3)

PLAN (from Blackboard):
[Read plan from Blackboard: redis-cli HGET artifacts:$PIPELINE_ID:plan_v1 data]

PAST LEARNINGS (Reflexion):
[Injected reflections from STEP 3]

REPOSITORY MAP:
[Compressed repo map from STEP 4]

REQUIREMENTS:
- Design bounded channel architecture (reflection: avoid unbounded channels)
- Plan graceful shutdown for 10K+ connections
- Error recovery strategy for network partitions

COORDINATION:
1. Read plan: redis-cli HGET artifacts:$PIPELINE_ID:plan_v1 data
2. Write your design: blackboard.write_artifact("architecture_design", content, PLAN)
3. Update status: redis-cli HSET status:$PIPELINE_ID phase "architecture_complete"

OUTPUT:
Provide architecture design document. Store on Blackboard.
  </parameter>
</invoke>
```

### 8.2 Parallel Agent Execution

**Independent tasks â†’ Parallel calls:**
```xml
<!-- Architecture + Tests can run in parallel -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">opus</parameter>
  <parameter name="prompt">@rust-architect: [architecture task]
  PIPELINE_ID: $PIPELINE_ID
  BLACKBOARD: $REDIS_URL</parameter>
</invoke>
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">@test-writer: [test generation task]
  PIPELINE_ID: $PIPELINE_ID
  BLACKBOARD: $REDIS_URL</parameter>
</invoke>
```

### 8.3 Blackboard Coordination Example

**Agent 1 (Architect) writes:**
```bash
# Agent writes architecture design to Blackboard
redis-cli -u $REDIS_URL HSET artifacts:$PIPELINE_ID:architecture_v1 \
    data '{
        "artifact_type": "PLAN",
        "content": "Use bounded mpsc channels with capacity 1000...",
        "created_by": "@rust-architect"
    }'

# Update status
redis-cli -u $REDIS_URL HSET status:$PIPELINE_ID \
    phase "architecture_complete" \
    progress "25"
```

**Agent 2 (Implementer) reads:**
```bash
# Agent reads architecture design from Blackboard
ARCHITECTURE=$(redis-cli -u $REDIS_URL HGET artifacts:$PIPELINE_ID:architecture_v1 data)

# Acquire lock before editing src/engine.rs
redis-cli -u $REDIS_URL SET locks:$PIPELINE_ID:src/engine.rs "@rust-pro" EX 300

# Implement based on architecture
# ...

# Write code artifact to Blackboard
redis-cli -u $REDIS_URL HSET artifacts:$PIPELINE_ID:code:src/engine.rs \
    data '{
        "artifact_type": "CODE",
        "content": "pub struct ConnectionPool { ... }",
        "created_by": "@rust-pro"
    }'

# Release lock
redis-cli -u $REDIS_URL DEL locks:$PIPELINE_ID:src/engine.rs
```

### 8.4 Agent Activity Logging

**All Blackboard operations logged:**
```bash
# Agent logs activity
redis-cli -u $REDIS_URL RPUSH agents:$PIPELINE_ID:@rust-pro \
    "$(date -Iseconds)|WRITE|artifacts:architecture_v1|success"

redis-cli -u $REDIS_URL RPUSH agents:$PIPELINE_ID:@rust-pro \
    "$(date -Iseconds)|LOCK|src/engine.rs|acquired"

redis-cli -u $REDIS_URL RPUSH agents:$PIPELINE_ID:@rust-pro \
    "$(date -Iseconds)|UNLOCK|src/engine.rs|released"
```

---

## STEP 9: Verification Gauntlet (in Shadow)

**Goal:** Comprehensive verification in isolated shadow workspace

### 9.1 Verification Stages

**5-stage gauntlet (all must pass):**
```
1. Syntax Check (AST parse)
2. Linter (ruff/eslint/clippy)
3. Type Check (mypy/tsc/cargo check)
4. Security Scan (bandit/CodeQL/cargo-audit)
5. Unit Tests (pytest/jest/cargo test)
```

### 9.2 Execute in Shadow Workspace

```bash
# Apply changes from Blackboard to shadow workspace
docker exec shadow-$PIPELINE_ID bash -c "
    cd /shadow-workspace

    # Retrieve code artifacts from Blackboard and apply
    # For each artifact in artifacts:$PIPELINE_ID:code:*
    # Apply changes to files
"

# Run verification gauntlet
docker exec shadow-$PIPELINE_ID bash -c "
    cd /shadow-workspace
    /usr/local/bin/verify
"
```

### 9.3 Verification Script (verify.sh in Docker)

```bash
#!/bin/bash
# Verification Gauntlet for Shadow Workspace

PROJECT_ROOT="/shadow-workspace"
cd $PROJECT_ROOT

echo "=== RVGB Verification Gauntlet ==="

# Detect project type
if [ -f "Cargo.toml" ]; then
    PROJECT_TYPE="rust"
elif [ -f "package.json" ]; then
    PROJECT_TYPE="node"
elif [ -f "setup.py" ] || [ -f "pyproject.toml" ]; then
    PROJECT_TYPE="python"
else
    PROJECT_TYPE="unknown"
fi

echo "Detected project type: $PROJECT_TYPE"

# Stage 1: Syntax Check
echo -e "\n[1/5] Syntax Check..."
case $PROJECT_TYPE in
    rust)
        cargo check --message-format=json 2>&1 | tee /tmp/syntax.log
        SYNTAX_RESULT=${PIPESTATUS[0]}
        ;;
    node)
        npx tsc --noEmit 2>&1 | tee /tmp/syntax.log
        SYNTAX_RESULT=${PIPESTATUS[0]}
        ;;
    python)
        python3 -m py_compile **/*.py 2>&1 | tee /tmp/syntax.log
        SYNTAX_RESULT=$?
        ;;
esac

if [ $SYNTAX_RESULT -ne 0 ]; then
    echo "âŒ SYNTAX CHECK FAILED"
    exit 1
fi
echo "âœ… Syntax check passed"

# Stage 2: Linter
echo -e "\n[2/5] Linter Check..."
case $PROJECT_TYPE in
    rust)
        cargo clippy --message-format=json 2>&1 | tee /tmp/lint.log
        LINT_RESULT=${PIPESTATUS[0]}
        ;;
    node)
        npx eslint . --format json > /tmp/lint.log 2>&1
        LINT_RESULT=$?
        ;;
    python)
        ruff check . --output-format=json > /tmp/lint.log 2>&1
        LINT_RESULT=$?
        ;;
esac

if [ $LINT_RESULT -ne 0 ]; then
    echo "âš ï¸  LINTER WARNINGS (non-blocking)"
fi
echo "âœ… Linter check complete"

# Stage 3: Type Check
echo -e "\n[3/5] Type Check..."
case $PROJECT_TYPE in
    rust)
        # Covered by cargo check
        echo "âœ… Type check covered by syntax check"
        ;;
    node)
        npx tsc --noEmit 2>&1 | tee /tmp/typecheck.log
        TYPE_RESULT=${PIPESTATUS[0]}
        ;;
    python)
        mypy . --json > /tmp/typecheck.log 2>&1
        TYPE_RESULT=$?
        ;;
esac

if [ ${TYPE_RESULT:-0} -ne 0 ]; then
    echo "âŒ TYPE CHECK FAILED"
    exit 3
fi
echo "âœ… Type check passed"

# Stage 4: Security Scan
echo -e "\n[4/5] Security Scan..."
case $PROJECT_TYPE in
    rust)
        cargo audit --json > /tmp/security.log 2>&1
        SEC_RESULT=$?
        ;;
    node)
        npm audit --json > /tmp/security.log 2>&1
        SEC_RESULT=$?
        ;;
    python)
        bandit -r . -f json -o /tmp/security.log 2>&1
        SEC_RESULT=$?
        ;;
esac

if [ $SEC_RESULT -ne 0 ]; then
    echo "âš ï¸  SECURITY WARNINGS (review required)"
fi
echo "âœ… Security scan complete"

# Stage 5: Unit Tests
echo -e "\n[5/5] Unit Tests..."
case $PROJECT_TYPE in
    rust)
        cargo test --message-format=json 2>&1 | tee /tmp/tests.log
        TEST_RESULT=${PIPESTATUS[0]}
        ;;
    node)
        npm test -- --json --outputFile=/tmp/tests.log 2>&1
        TEST_RESULT=$?
        ;;
    python)
        pytest --json-report --json-report-file=/tmp/tests.log 2>&1
        TEST_RESULT=$?
        ;;
esac

if [ $TEST_RESULT -ne 0 ]; then
    echo "âŒ TESTS FAILED"
    exit 5
fi
echo "âœ… All tests passed"

echo -e "\n=== âœ… VERIFICATION GAUNTLET PASSED ==="
exit 0
```

### 9.4 Parse Verification Results

```bash
# Extract results from shadow container
docker cp shadow-$PIPELINE_ID:/tmp/syntax.log /tmp/shadow-results/
docker cp shadow-$PIPELINE_ID:/tmp/lint.log /tmp/shadow-results/
docker cp shadow-$PIPELINE_ID:/tmp/typecheck.log /tmp/shadow-results/
docker cp shadow-$PIPELINE_ID:/tmp/security.log /tmp/shadow-results/
docker cp shadow-$PIPELINE_ID:/tmp/tests.log /tmp/shadow-results/

# Check exit code
VERIFICATION_EXIT_CODE=$?
```

### 9.5 Verification Report

```markdown
## Verification Gauntlet Results

| Stage | Status | Details |
|-------|--------|---------|
| 1. Syntax Check | âœ… PASS | No syntax errors |
| 2. Linter | âš ï¸ WARN | 3 warnings (non-blocking) |
| 3. Type Check | âœ… PASS | All types valid |
| 4. Security Scan | âœ… PASS | No vulnerabilities |
| 5. Unit Tests | âœ… PASS | 47/47 tests passed |

**Overall:** âœ… PASSED (all critical stages passed)

**Warnings (non-blocking):**
- Linter: Unused import in src/utils.rs:15
- Linter: Consider using if-let instead of match in src/engine.rs:230
- Linter: Function complexity high in src/engine.rs:process_order

**Exit Code:** 0
```

**If verification fails:**
```markdown
## Verification Gauntlet Results

| Stage | Status | Details |
|-------|--------|---------|
| 1. Syntax Check | âœ… PASS | No syntax errors |
| 2. Linter | âœ… PASS | No issues |
| 3. Type Check | âŒ FAIL | Type mismatch in src/engine.rs:145 |
| 4. Security Scan | â­ï¸ SKIP | Skipped due to type check failure |
| 5. Unit Tests | â­ï¸ SKIP | Skipped due to type check failure |

**Overall:** âŒ FAILED (type check failed)

**Errors:**
```
error[E0308]: mismatched types
  --> src/engine.rs:145:20
   |
145 |         let result = process_async(order).await;
   |                      ^^^^^^^^^^^^^^^^^^^^^^^^ expected `Result<Fill>`, found `Fill`
```

**Next Step:** Feed error to agent for retry (TDD Loop - STEP 10)
```

---

## STEP 10: TDD Loop (3-strike max)

**Goal:** Iterative fix loop with maximum 3 attempts

### 10.1 Strike System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STRIKE 1: Initial Verification                             â”‚
â”‚  â”œâ”€ Run verification gauntlet (STEP 9)                      â”‚
â”‚  â”œâ”€ If PASS â†’ Continue to STEP 11                           â”‚
â”‚  â””â”€ If FAIL â†’ Extract errors, proceed to Strike 2           â”‚
â”‚                                                             â”‚
â”‚  STRIKE 2: Agent Fix Attempt                                â”‚
â”‚  â”œâ”€ Feed errors to original agent                           â”‚
â”‚  â”œâ”€ Agent fixes in shadow workspace (via Blackboard)        â”‚
â”‚  â”œâ”€ Re-run verification gauntlet                            â”‚
â”‚  â”œâ”€ If PASS â†’ Continue to STEP 11                           â”‚
â”‚  â””â”€ If FAIL â†’ Proceed to Strike 3                           â”‚
â”‚                                                             â”‚
â”‚  STRIKE 3: Expert Escalation                                â”‚
â”‚  â”œâ”€ Spawn @code-reviewer to analyze root cause              â”‚
â”‚  â”œâ”€ Reviewer provides fix recommendations                   â”‚
â”‚  â”œâ”€ Original agent applies fix                              â”‚
â”‚  â”œâ”€ Re-run verification gauntlet                            â”‚
â”‚  â”œâ”€ If PASS â†’ Continue to STEP 11                           â”‚
â”‚  â””â”€ If FAIL â†’ HALT, rollback shadow, report to user         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 Strike 2: Agent Fix

**Extract error context:**
```bash
# Parse verification logs for specific errors
SYNTAX_ERRORS=$(jq -r '.errors[]' /tmp/shadow-results/syntax.log)
TYPE_ERRORS=$(jq -r '.errors[]' /tmp/shadow-results/typecheck.log)
TEST_FAILURES=$(jq -r '.failures[]' /tmp/shadow-results/tests.log)
```

**Feed errors to agent:**
```xml
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @rust-pro. Fix the failing verification in shadow workspace.

PIPELINE_ID: $PIPELINE_ID
BLACKBOARD: $REDIS_URL
SHADOW WORKSPACE: shadow-$PIPELINE_ID (Docker container)

VERIFICATION FAILURES:
```
[Type Check Errors]
error[E0308]: mismatched types
  --> src/engine.rs:145:20
   |
145 |         let result = process_async(order).await;
   |                      ^^^^^^^^^^^^^^^^^^^^^^^^ expected `Result<Fill>`, found `Fill`
```

EXPECTED: process_async should return Result<Fill>
ACTUAL: process_async returns Fill directly

ROOT CAUSE: Missing error handling wrapper

FIX REQUIRED:
1. Read current code from Blackboard: artifacts:$PIPELINE_ID:code:src/engine.rs
2. Wrap process_async call with Ok() or add ? operator
3. Update code artifact on Blackboard
4. Orchestrator will re-run verification

Fix this specific error. Do not refactor unrelated code.
  </parameter>
</invoke>
```

### 10.3 Strike 3: Expert Escalation

**If Strike 2 fails, get expert analysis:**
```xml
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">opus</parameter>
  <parameter name="prompt">You are @code-reviewer, expert debugging specialist.

CONTEXT: Agent has failed to fix verification error after 2 attempts.

ORIGINAL ERROR (Strike 1):
```
error[E0308]: mismatched types in src/engine.rs:145
```

ATTEMPTED FIX (Strike 2):
[What agent tried]

PERSISTING ERROR (Strike 2 verification):
```
error[E0308]: still mismatched types in src/engine.rs:145
[possibly different details]
```

CODE CONTEXT:
[Read from Blackboard: artifacts:$PIPELINE_ID:code:src/engine.rs]

ANALYSIS REQUIRED:
1. Why did the first fix fail?
2. What is the actual root cause (not symptoms)?
3. What is the correct fix?

Provide detailed root cause analysis and specific fix instructions.
  </parameter>
</invoke>
```

**Apply expert fix:**
```xml
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @rust-pro. Apply the expert reviewer's fix.

EXPERT ANALYSIS:
[Reviewer's root cause analysis]

RECOMMENDED FIX:
[Specific instructions from reviewer]

TASK:
1. Read code from Blackboard
2. Apply the exact fix recommended
3. Update Blackboard artifact
4. Do NOT make additional changes

This is Strike 3 (final attempt). Must succeed.
  </parameter>
</invoke>
```

### 10.4 TDD Loop Exhausted

**If Strike 3 fails:**
```markdown
## âš ï¸ TDD LOOP EXHAUSTED (3/3 strikes)

**Strike 1 (Initial):**
- Error: Type mismatch in src/engine.rs:145
- Details: Expected `Result<Fill>`, found `Fill`

**Strike 2 (Agent Fix):**
- Attempted: Wrapped with Ok()
- Result: Still failed - different type error
- New error: Expected `Result<Fill, Error>`, found `Result<Fill, ()>`

**Strike 3 (Expert Escalation):**
- Expert analysis: Missing error type in function signature
- Attempted: Changed return type to Result<Fill, Error>
- Result: Still failed - Error type not in scope
- Final error: Cannot find type `Error` in this scope

**RECOMMENDATION:**
This appears to be a deeper architectural issue requiring manual intervention:
1. Error type needs to be defined/imported
2. May need to refactor error handling strategy
3. Consider using thiserror or anyhow crate

**SHADOW WORKSPACE STATUS:**
- Container: shadow-$PIPELINE_ID (kept for debugging)
- Branch: rollback-$PIPELINE_ID (preserved)
- Main workspace: UNCHANGED (protected)

**NEXT STEPS:**
- Manual debugging recommended
- Review error handling architecture
- Consider adding thiserror dependency
```

**Store failure in procedural_memory for learning:**
```sql
INSERT INTO procedural_memory (reflection_text, failure_context, relevance_tags)
VALUES (
    'When encountering persistent type errors in Rust async code, check that error types are properly defined and in scope. Using error handling crates like thiserror or anyhow can prevent this.',
    '{"what_failed": "Type error in async function after 3 fix attempts",
      "root_cause": "Error type not defined/imported in scope",
      "fix_applied": "Would require defining custom Error type or using error crate",
      "lesson": "Define error types early in Rust projects"}',
    ARRAY['rust', 'async', 'error-handling', 'types']
);
```

---

## STEP 11: Multi-Perspective Review

**Goal:** Multiple specialists review the working code in parallel

**Only if verification passed in STEP 9 or 10.**

### 11.1 Parallel Review Agents

**Spawn 5 reviewers simultaneously:**
```xml
<!-- Security Review -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @security-auditor, expert in security vulnerabilities.

TASK: Review code for security issues.

CODE LOCATION:
- Read from Blackboard: artifacts:$PIPELINE_ID:code:*
- Or inspect shadow workspace: shadow-$PIPELINE_ID:/shadow-workspace

FOCUS AREAS:
- Input validation
- SQL injection / command injection
- Authentication/authorization
- Sensitive data exposure
- Cryptographic issues
- Dependencies with known CVEs

OUTPUT:
Rate each area: ğŸ”´ Critical / ğŸŸ¡ Warning / ğŸŸ¢ Good
Provide specific findings with line numbers.
  </parameter>
</invoke>

<!-- Architecture Review -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @architecture-reviewer, expert in system design.

TASK: Review code for architectural quality.

FOCUS AREAS:
- Modularity and separation of concerns
- Coupling and cohesion
- Scalability considerations
- Error handling strategy
- Async/concurrency patterns

OUTPUT:
Rate: ğŸ”´ Poor / ğŸŸ¡ Acceptable / ğŸŸ¢ Excellent
Suggest improvements.
  </parameter>
</invoke>

<!-- Performance Review -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @performance-reviewer, expert in optimization.

TASK: Review code for performance bottlenecks.

FOCUS AREAS:
- Algorithmic complexity
- Memory allocations
- Unnecessary cloning/copying
- Lock contention
- Database query efficiency

OUTPUT:
Rate: ğŸ”´ Bottleneck / ğŸŸ¡ Suboptimal / ğŸŸ¢ Optimized
Identify hot paths.
  </parameter>
</invoke>

<!-- Simplicity Review -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">haiku</parameter>
  <parameter name="prompt">You are @simplicity-reviewer, expert in clean code.

TASK: Review code for over-engineering and complexity.

FOCUS AREAS:
- YAGNI violations (over-engineering)
- Code readability
- Naming clarity
- Documentation adequacy
- Dead code or unnecessary abstractions

OUTPUT:
Rate: ğŸ”´ Over-engineered / ğŸŸ¡ Complex / ğŸŸ¢ Simple
Suggest simplifications.
  </parameter>
</invoke>

<!-- Code Quality Review -->
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @code-reviewer, expert in code quality.

TASK: General code review for quality and best practices.

FOCUS AREAS:
- Idiomatic code for the language
- Error handling completeness
- Test coverage adequacy
- Edge case handling
- Code duplication

OUTPUT:
Rate: ğŸ”´ Needs work / ğŸŸ¡ Acceptable / ğŸŸ¢ High quality
List specific improvements.
  </parameter>
</invoke>
```

### 11.2 Aggregate Review Results

**Collect reviews from Blackboard:**
```bash
# Reviewers write to Blackboard
# artifacts:$PIPELINE_ID:review:security
# artifacts:$PIPELINE_ID:review:architecture
# artifacts:$PIPELINE_ID:review:performance
# artifacts:$PIPELINE_ID:review:simplicity
# artifacts:$PIPELINE_ID:review:quality

# Read all reviews
for review_type in security architecture performance simplicity quality; do
    redis-cli -u $REDIS_URL HGET artifacts:$PIPELINE_ID:review:$review_type data
done
```

### 11.3 Review Summary

```markdown
## Multi-Perspective Review Summary

| Reviewer | Rating | Critical Issues | Warnings | Notes |
|----------|--------|-----------------|----------|-------|
| Security | ğŸŸ¢ Good | 0 | 1 | Consider rate limiting on WebSocket endpoint |
| Architecture | ğŸŸ¢ Excellent | 0 | 0 | Clean separation, good error handling |
| Performance | ğŸŸ¡ Acceptable | 0 | 2 | Clone in hot path (line 145), consider Arc |
| Simplicity | ğŸŸ¢ Simple | 0 | 1 | Could extract helper function for validation |
| Code Quality | ğŸŸ¢ High Quality | 0 | 0 | Idiomatic Rust, good test coverage |

**Overall Assessment:** âœ… APPROVED (no critical issues)

**Detailed Findings:**

### Security (ğŸŸ¢ Good)
- âœ… Input validation present
- âœ… No SQL injection risks (using ORM)
- âœ… Authentication enforced
- âš ï¸  **Warning:** WebSocket endpoint lacks rate limiting (non-blocking)
  - Location: src/websocket.rs:89
  - Recommendation: Add per-client message rate limit

### Architecture (ğŸŸ¢ Excellent)
- âœ… Clean separation of concerns
- âœ… Error handling via Result types
- âœ… Bounded channels prevent memory leaks
- âœ… Graceful shutdown implemented

### Performance (ğŸŸ¡ Acceptable)
- âš ï¸  **Warning:** Unnecessary clone in hot path
  - Location: src/engine.rs:145
  - Issue: `order.clone()` on every iteration
  - Recommendation: Use Arc<Order> or borrow
- âš ï¸  **Warning:** Lock held across await point
  - Location: src/engine.rs:230
  - Recommendation: Minimize lock scope

### Simplicity (ğŸŸ¢ Simple)
- âœ… Readable, idiomatic code
- âœ… No over-engineering
- âš ï¸  **Suggestion:** Extract validation logic to helper
  - Location: src/websocket.rs:120-140 (20 lines of validation)
  - Recommendation: Create `validate_order()` function

### Code Quality (ğŸŸ¢ High Quality)
- âœ… Follows Rust best practices
- âœ… Comprehensive error handling
- âœ… Good test coverage (47 tests)
- âœ… No code duplication
```

### 11.4 Critical Issues â†’ Block Merge

**If any reviewer finds ğŸ”´ Critical issues:**
```markdown
## âš ï¸ REVIEW BLOCKED - CRITICAL ISSUES FOUND

**Security Review: ğŸ”´ Critical**
- **CRITICAL:** SQL Injection vulnerability in query builder
  - Location: src/db.rs:56
  - Issue: User input concatenated directly into SQL query
  - Fix Required: Use parameterized queries
  - **BLOCKING:** Cannot merge until fixed

**Next Steps:**
1. Spawn agent to fix critical security issue
2. Re-run verification gauntlet
3. Re-run security review
4. Proceed only if all critical issues resolved
```

---

## STEP 12: Reflexion - Generate & Store

**Goal:** Generate reflection on the process and store learnings

### 12.1 Self-Critique Checklist

**Evaluate the completed work:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REFLEXION SELF-CRITIQUE                                    â”‚
â”‚  â”œâ”€ Does solution actually solve the stated problem? âœ…/âŒ  â”‚
â”‚  â”œâ”€ Are there obvious bugs or edge cases missed? âœ…/âŒ      â”‚
â”‚  â”œâ”€ Is it over-engineered for the task? âœ…/âŒ               â”‚
â”‚  â”œâ”€ Did reviewers flag critical issues? âœ…/âŒ               â”‚
â”‚  â”œâ”€ Would I be confident deploying this? âœ…/âŒ              â”‚
â”‚  â””â”€ What could have been done better? [free text]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.2 Generate Reflection (if issues found)

**If any failures occurred during pipeline:**
```xml
<invoke name="Task">
  <parameter name="subagent_type">general-purpose</parameter>
  <parameter name="model">sonnet</parameter>
  <parameter name="prompt">You are @reflection-generator, expert in learning from experience.

TASK: Generate reflection on the orchestration process.

PIPELINE CONTEXT:
- Task: Implement WebSocket connection pool
- Outcome: Success (after 2 TDD strikes)
- Failures encountered: Type error in Strike 1, Error type scope in Strike 2

FAILURE DETAILS:
**Strike 1:** Type mismatch - expected Result<Fill>, found Fill
- Root cause: Missing error handling wrapper
- Fix: Added ? operator

**Strike 2:** Error type not in scope
- Root cause: Custom Error type not imported
- Fix: Added `use crate::errors::Error;`

GENERATE REFLECTION:
1. What was the core issue?
2. Why did it happen?
3. How was it resolved?
4. What lesson should be learned?
5. What tags are relevant? (for future retrieval)

OUTPUT FORMAT:
```json
{
  "reflection_text": "One-sentence key learning",
  "failure_context": {
    "what_failed": "...",
    "root_cause": "...",
    "fix_applied": "...",
    "lesson": "..."
  },
  "relevance_tags": ["tag1", "tag2", ...]
}
```
  </parameter>
</invoke>
```

### 12.3 Store Reflection in Procedural Memory

**Insert reflection into PostgreSQL:**
```sql
INSERT INTO procedural_memory (
    reflection_text,
    failure_context,
    relevance_tags,
    created_at
)
VALUES (
    'When implementing async Rust functions that can error, always define and import error types before using Result<T, E>',
    '{
        "what_failed": "Type errors in async function after initial implementation",
        "root_cause": "Error type not defined/imported in function signature scope",
        "fix_applied": "Added custom Error type definition and imported in module",
        "lesson": "Define error types early in module hierarchy, before implementing functions that use them"
    }'::jsonb,
    ARRAY['rust', 'async', 'error-handling', 'types', 'result'],
    NOW()
);
```

**Using MCP (if available):**
```
mcp__memory__store_reflection(
    reflection="When implementing async Rust functions that can error, always define and import error types before using Result<T, E>",
    failure_context={
        "what_failed": "Type errors in async function",
        "root_cause": "Error type not in scope",
        "fix_applied": "Defined and imported Error type",
        "lesson": "Define error types early"
    },
    tags=["rust", "async", "error-handling", "types"]
)
```

### 12.4 Link Reflection to Knowledge Graph

**Update knowledge graph with reflection:**
```sql
-- Create entity for the reflection
INSERT INTO entities (entity_type, entity_name, metadata)
VALUES (
    'reflection',
    'rust-async-error-types',
    '{"topic": "error handling", "language": "rust"}'::jsonb
)
ON CONFLICT (entity_type, entity_name) DO UPDATE
SET last_modified = NOW();

-- Link reflection to related entities
-- (e.g., link to src/engine.rs file entity)
INSERT INTO relations (source_entity_id, relation_type, target_entity_id)
SELECT
    (SELECT id FROM entities WHERE entity_type='reflection' AND entity_name='rust-async-error-types'),
    'applies_to',
    (SELECT id FROM entities WHERE entity_type='file' AND entity_name='src/engine.rs')
ON CONFLICT DO NOTHING;
```

### 12.5 Reflexion Summary

```markdown
## Reflexion Generated

**Reflection:** When implementing async Rust functions that can error, always define and import error types before using Result<T, E>

**Context:**
- What failed: Type errors in async function implementation
- Root cause: Error type not defined/imported in scope
- Fix applied: Defined custom Error type and imported in module
- Lesson: Define error types early in module hierarchy

**Stored:**
- âœ… Procedural memory (PostgreSQL)
- âœ… Knowledge graph link (reflection â†’ src/engine.rs)
- âœ… Tags: rust, async, error-handling, types, result

**Future Impact:**
Next time a similar task is detected (async Rust + error handling), this reflection will be retrieved and injected into agent context (STEP 3).
```

**If no significant issues:**
```markdown
## Reflexion Summary

No significant failures encountered during pipeline.
Routine success - no new reflection generated.

**Process Quality:**
- âœ… First-time verification pass
- âœ… All reviews green
- âœ… No critical issues
```

---

## STEP 13: Commit Shadow or Rollback

**Goal:** Finalize changes by committing shadow workspace to main, or rollback on failure

### 13.1 Decision Logic

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMIT CONDITIONS (all must be true):                      â”‚
â”‚  â”œâ”€ âœ… Verification gauntlet passed                         â”‚
â”‚  â”œâ”€ âœ… TDD loop succeeded (within 3 strikes)                â”‚
â”‚  â”œâ”€ âœ… No ğŸ”´ critical review findings                       â”‚
â”‚  â””â”€ âœ… Reflexion check passed                               â”‚
â”‚                                                             â”‚
â”‚  IF all true â†’ COMMIT shadow to main                        â”‚
â”‚  IF any false â†’ ROLLBACK (discard shadow)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.2 Commit Shadow to Main

**Extract changes from shadow container:**
```bash
# Get diff from shadow workspace
docker exec shadow-$PIPELINE_ID bash -c "
    cd /shadow-workspace
    git diff HEAD
" > /tmp/shadow-changes.patch

# Apply diff to main workspace
cd $PROJECT_ROOT
git apply /tmp/shadow-changes.patch

# Verify changes applied correctly
git status
git diff

# Commit changes
git add -A
git commit -m "$(cat <<'EOF'
Implement WebSocket connection pool with bounded channels

- Added ConnectionPool struct with configurable capacity
- Implemented graceful shutdown for 10K+ connections
- Added error recovery for network partitions
- Comprehensive test coverage (47 tests)

Verification:
- âœ… All tests passing
- âœ… Security review: No critical issues
- âœ… Performance review: Acceptable (2 minor optimizations suggested)

RVGB Pipeline: $PIPELINE_ID

ğŸ¤– Generated with RVGB v3.0
Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

### 13.3 Cleanup Shadow Container

```bash
# Stop and remove shadow container
docker stop shadow-$PIPELINE_ID
docker rm shadow-$PIPELINE_ID

# Clean up shadow files
rm -rf /tmp/shadow-results/
rm -f /tmp/shadow-changes.patch

echo "âœ… Shadow workspace committed and cleaned up"
```

### 13.4 Rollback Shadow

**If any critical failures:**
```bash
# Discard shadow container (all changes lost)
docker stop shadow-$PIPELINE_ID
docker rm -f shadow-$PIPELINE_ID

# Clean up
rm -rf /tmp/shadow-results/

echo "âš ï¸  Shadow workspace rolled back. Main workspace unchanged."
```

### 13.5 Commit/Rollback Report

**Success (Commit):**
```markdown
## Shadow Workspace Status: âœ… COMMITTED

**Changes applied to main workspace:**
- Modified: src/engine.rs (+120 lines)
- Modified: src/websocket.rs (+85 lines)
- Added: src/connection_pool.rs (new file, 200 lines)
- Modified: tests/engine_test.rs (+30 lines)

**Verification before commit:**
- âœ… All tests passing (47/47)
- âœ… No syntax errors
- âœ… No type errors
- âœ… Security scan clean
- âœ… All reviews approved

**Commit hash:** a1b2c3d4

**Shadow container:** Cleaned up
```

**Failure (Rollback):**
```markdown
## Shadow Workspace Status: âš ï¸  ROLLED BACK

**Reason:** TDD loop exhausted (3 strikes) - persistent type errors

**Main workspace:** UNCHANGED (protected)

**Shadow preserved for debugging:**
- Container ID: shadow-$PIPELINE_ID (stopped)
- To inspect: `docker start shadow-$PIPELINE_ID && docker exec -it shadow-$PIPELINE_ID bash`
- To discard: `docker rm -f shadow-$PIPELINE_ID`

**Recommendation:**
Manual intervention required. Review error handling architecture.
```

---

## STEP 14: Memory Consolidation ("Sleep")

**Goal:** Consolidate episodic memories into semantic knowledge, update knowledge graph

### 14.1 Trigger Consolidation

**Run consolidation process (simulates "sleep cycle"):**
```sql
-- Call consolidation function with pipeline ID
SELECT consolidate_pipeline_memory('$PIPELINE_ID');
```

**Using MCP (if available):**
```
mcp__memory__consolidate_memory(pipeline_run_id='$PIPELINE_ID')
```

### 14.2 Consolidation Process

**What happens during consolidation:**

**14.2.1 Extract Entities from Episodes**
```sql
-- Find all code artifacts created during pipeline
SELECT
    event_type,
    agent_name,
    context->>'file_path' AS file_path,
    context->>'function_name' AS function_name
FROM episodic_memory
WHERE
    pipeline_run_id = '$PIPELINE_ID'
    AND event_type IN ('code_created', 'code_modified');

-- Insert entities into knowledge graph
INSERT INTO entities (entity_type, entity_name, metadata)
VALUES
    ('file', 'src/connection_pool.rs', '{"created_by": "@rust-pro"}'::jsonb),
    ('function', 'ConnectionPool::new', '{"file": "src/connection_pool.rs"}'::jsonb),
    ('function', 'ConnectionPool::acquire', '{"file": "src/connection_pool.rs"}'::jsonb)
ON CONFLICT (entity_type, entity_name) DO UPDATE
SET last_modified = NOW();
```

**14.2.2 Extract Relations**
```sql
-- Identify relations from code analysis
-- (e.g., src/websocket.rs uses ConnectionPool)
INSERT INTO relations (source_entity_id, relation_type, target_entity_id, weight)
SELECT
    (SELECT id FROM entities WHERE entity_type='file' AND entity_name='src/websocket.rs'),
    'uses',
    (SELECT id FROM entities WHERE entity_type='file' AND entity_name='src/connection_pool.rs'),
    1.0
ON CONFLICT (source_entity_id, relation_type, target_entity_id) DO UPDATE
SET
    weight = relations.weight + 0.1,
    last_seen = NOW();
```

**14.2.3 Update Semantic Patterns**
```sql
-- Check if this pipeline used any existing patterns
-- Update usage stats
UPDATE semantic_memory
SET
    times_used = times_used + 1,
    last_used = NOW()
WHERE
    category = 'rust-async-websockets'
    AND pattern_name ILIKE '%connection-pool%';

-- If new pattern detected, insert
INSERT INTO semantic_memory (
    pattern_name,
    category,
    description,
    key_elements,
    success_rate,
    times_used
)
VALUES (
    'bounded-channel-connection-pool',
    'rust-async-websockets',
    'Connection pool using bounded channels to prevent memory exhaustion',
    '{"channels": "bounded", "capacity": 1000, "graceful_shutdown": true}'::jsonb,
    1.0,
    1
)
ON CONFLICT (pattern_name) DO UPDATE
SET times_used = semantic_memory.times_used + 1;
```

**14.2.4 Archive Old Episodes**
```sql
-- Archive episodes older than 30 days to cold storage
-- (Keep in DB but mark as archived, or move to separate table)
UPDATE episodic_memory
SET metadata = jsonb_set(
    COALESCE(metadata, '{}'::jsonb),
    '{archived}',
    'true'
)
WHERE
    timestamp < NOW() - INTERVAL '30 days'
    AND importance < 0.5;
```

**14.2.5 Prune Low-Utility Patterns**
```sql
-- Every 10 pipeline runs, prune patterns with utility_score < 0.3
-- (Only run if total_orchestrations % 10 == 0)

-- Archive low-utility patterns
DELETE FROM semantic_memory
WHERE
    utility_score < 0.3
    AND times_used < 2
    AND last_used < NOW() - INTERVAL '60 days';
```

### 14.3 Knowledge Graph Update

**Updated graph structure:**
```
Entities:
- file:src/connection_pool.rs (NEW)
- file:src/websocket.rs (UPDATED)
- file:src/engine.rs (UPDATED)
- function:ConnectionPool::new (NEW)
- function:ConnectionPool::acquire (NEW)
- reflection:rust-async-error-types (NEW)

Relations:
- src/websocket.rs --uses--> src/connection_pool.rs (NEW)
- src/engine.rs --uses--> src/connection_pool.rs (NEW)
- reflection:rust-async-error-types --applies_to--> src/engine.rs (NEW)
- ConnectionPool::new --called_by--> src/websocket.rs (NEW)
```

### 14.4 Consolidation Summary

```markdown
## Memory Consolidation Complete

**Episodic Memory:**
- New episodes: 23 (from this pipeline)
- Total episodes: 1,270
- Archived episodes: 145 (>30 days old, low importance)

**Semantic Memory:**
- Patterns updated: 2
  - rust-async-websockets (times_used: 12 â†’ 13)
  - error-handling-async (times_used: 15 â†’ 16)
- New patterns: 1
  - bounded-channel-connection-pool (utility: 0.7)
- Patterns pruned: 0 (next prune at 10-run interval)

**Knowledge Graph:**
- New entities: 5 (3 functions, 1 file, 1 reflection)
- New relations: 4
- Updated relations: 2 (weight increased)
- Total entities: 487
- Total relations: 1,203

**Procedural Memory (Reflections):**
- New reflections: 1 (rust async error handling)
- Total reflections: 34

**Next Consolidation:** After 9 more pipeline runs (every 10 runs: prune low-utility patterns)
```

---

## STEP 15: Report to User

**Goal:** Comprehensive final report on RVGB orchestration

### 15.1 Full Report Template

```markdown
# RVGB Orchestration Complete v3.0

## Task
**Requested:** Implement WebSocket connection pool with bounded channels for high-throughput order matching

**Duration:** 8 minutes 32 seconds
**Pipeline ID:** rvgb-1732880400-a1b2c3

---

## Infrastructure Status

| Component | Status | Details |
|-----------|--------|---------|
| PostgreSQL | âœ… Connected | 5 tables, 1,270 episodes |
| Redis Blackboard | âœ… Connected | 47 artifacts written |
| Docker Shadow | âœ… Running | Container: shadow-rvgb-1732880400-a1b2c3 |
| MCP Memory Server | âš ï¸ Not installed | Fallback: Direct PostgreSQL queries |

---

## Memory Retrieval Stats

**Episodic Memory:**
- Retrieved: 7 episodes (avg relevance score: 0.82)
- Most relevant: "WebSocket connection pooling fix" (score: 0.94)

**Semantic Memory:**
- Patterns loaded: 4 (avg utility: 0.81)
  - rust-async-websockets (utility: 0.91, used: 12â†’13x)
  - order-matching-algorithms (utility: 0.87)
  - error-handling-async (utility: 0.76, used: 15â†’16x)
  - docker-compose-setup (utility: 0.68)

**Reflections:**
- Retrieved: 3 past reflections
- Applied: "Always bound async channels" (applied 2â†’3x)

---

## Context Compression

**Repository Map:**
- Files scanned: 47
- Files loaded: 12 (compressed to signatures)
- Files summarized: 8
- Files skipped: 27 (tests, configs)
- **Token budget:** 12,450 / 50,000 (24% used)

---

## Blackboard Operations

**Pipeline ID:** rvgb-1732880400-a1b2c3

| Operation | Count |
|-----------|-------|
| Artifacts written | 47 |
| Artifacts read | 89 |
| Locks acquired | 12 |
| Locks released | 12 |
| Status updates | 18 |
| Agent logs | 156 |

**Top Artifacts:**
- plan_v1 (PLAN) - read 15x
- architecture_v1 (PLAN) - read 8x
- code:src/connection_pool.rs (CODE) - updated 3x
- code:src/websocket.rs (CODE) - updated 2x

---

## Implementation

**Model Routing:**

| Agent | Task | Model | Status | Duration |
|-------|------|-------|--------|----------|
| @rust-architect | Design connection pool architecture | opus | âœ… | 2m 15s |
| @rust-pro | Implement ConnectionPool struct | sonnet | âœ… | 3m 40s |
| @rust-pro | Implement WebSocket handler | sonnet | âœ… | 2m 50s |
| @test-writer | Write unit tests | sonnet | âœ… | 1m 30s |
| @test-writer | Write integration tests | sonnet | âœ… | 2m 10s |
| @rust-pro | Add debug logging | haiku | âœ… | 0m 25s |
| @rust-pro | Format code | haiku | âœ… | 0m 10s |
| @rust-pro | Update config examples | haiku | âœ… | 0m 18s |

**Model Cost Breakdown:**

| Model | Tasks | Tokens In | Tokens Out | Est. Cost |
|-------|-------|-----------|------------|-----------|
| opus | 2 | 12,450 | 8,230 | $0.28 |
| sonnet | 5 | 28,900 | 15,670 | $0.14 |
| haiku | 3 | 6,200 | 1,450 | $0.002 |
| **Total** | **10** | **47,550** | **25,350** | **$0.42** |

---

## Shadow Workspace Verification

**Container:** shadow-rvgb-1732880400-a1b2c3

### Verification Gauntlet Results

| Stage | Status | Details |
|-------|--------|---------|
| 1. Syntax Check | âœ… PASS | No syntax errors |
| 2. Linter (clippy) | âš ï¸ WARN | 3 warnings (non-blocking) |
| 3. Type Check | âœ… PASS | All types valid |
| 4. Security Scan | âœ… PASS | No vulnerabilities (cargo-audit) |
| 5. Unit Tests | âœ… PASS | 47/47 tests passed (100% coverage) |

**Overall:** âœ… PASSED

**Warnings (non-blocking):**
- Unused import in src/utils.rs:15
- Consider using if-let in src/engine.rs:230
- Function complexity high in src/engine.rs:process_order

---

## TDD Loop Results

| Strike | Action | Result | Duration |
|--------|--------|--------|----------|
| 1 | Initial verification | âŒ FAIL | - |
|   | Error: Type mismatch in src/engine.rs:145 | | |
| 2 | Agent fix attempt (@rust-pro) | âœ… PASS | 1m 20s |
|   | Fix: Added ? operator for error propagation | | |

**Total Attempts:** 2/3 (succeeded on Strike 2)

---

## Multi-Perspective Review

| Reviewer | Rating | Critical | Warnings | Notes |
|----------|--------|----------|----------|-------|
| @security-auditor | ğŸŸ¢ Good | 0 | 1 | Consider rate limiting on WS endpoint |
| @architecture-reviewer | ğŸŸ¢ Excellent | 0 | 0 | Clean separation, good error handling |
| @performance-reviewer | ğŸŸ¡ Acceptable | 0 | 2 | Clone in hot path (line 145) |
| @simplicity-reviewer | ğŸŸ¢ Simple | 0 | 1 | Extract validation helper |
| @code-reviewer | ğŸŸ¢ High Quality | 0 | 0 | Idiomatic Rust, good coverage |

**Overall Assessment:** âœ… APPROVED (no critical issues)

**Detailed Findings:**
- Security: Rate limiting suggestion (src/websocket.rs:89)
- Performance: Unnecessary clone (src/engine.rs:145) - consider Arc
- Simplicity: Extract validation function (src/websocket.rs:120-140)

---

## Reflexion

**Self-Critique:** âœ… All checks passed
- âœ… Solves stated problem (connection pool with bounded channels)
- âœ… No obvious bugs (47 tests passing)
- âœ… Not over-engineered (simplicity review: ğŸŸ¢)
- âœ… No critical review findings
- âœ… Confident to deploy

**Reflection Generated:**

**Learning:** When implementing async Rust functions that can error, always define and import error types before using Result<T, E>

**Context:**
- What failed: Type errors in async function (Strike 1)
- Root cause: Missing error type import
- Fix applied: Added `use crate::errors::Error;`
- Lesson: Define error types early in module hierarchy

**Stored in:**
- âœ… Procedural memory (PostgreSQL)
- âœ… Knowledge graph (reflection â†’ src/engine.rs)
- âœ… Tags: rust, async, error-handling, types, result

---

## Shadow Workspace Status

**Status:** âœ… COMMITTED to main workspace

**Changes Applied:**
- Modified: src/engine.rs (+120 lines)
- Modified: src/websocket.rs (+85 lines)
- Added: src/connection_pool.rs (new file, 200 lines)
- Modified: tests/engine_test.rs (+30 lines)

**Commit Hash:** a1b2c3d4e5f6

**Shadow Container:** Cleaned up (stopped and removed)

---

## Memory Consolidation

**Consolidation completed at:** 2025-11-29T10:38:32Z

### Episodic Memory
- New episodes: 23 (from this pipeline)
- Total episodes: 1,270
- Archived: 145 (>30 days, low importance)

### Semantic Memory
- Patterns updated: 2 (rust-async-websockets, error-handling-async)
- New patterns: 1 (bounded-channel-connection-pool, utility: 0.7)
- Patterns pruned: 0 (next prune: 9 runs from now)

### Knowledge Graph
- New entities: 5 (3 functions, 1 file, 1 reflection)
- New relations: 4
- Updated relations: 2 (weight increased)
- **Total graph size:** 487 entities, 1,203 relations

### Procedural Memory (Reflections)
- New reflections: 1 (rust async error handling)
- Total reflections: 34

---

## Knowledge Graph Updates

**New Entities:**
- file:src/connection_pool.rs
- function:ConnectionPool::new
- function:ConnectionPool::acquire
- function:ConnectionPool::release
- reflection:rust-async-error-types

**New Relations:**
- src/websocket.rs --uses--> src/connection_pool.rs
- src/engine.rs --uses--> src/connection_pool.rs
- reflection:rust-async-error-types --applies_to--> src/engine.rs
- ConnectionPool::new --called_by--> src/websocket.rs

**Graph Insights:**
- Most central file: src/engine.rs (15 relations)
- Most reused module: src/types.rs (imported by 8 files)

---

## Final Summary

âœ… **Task completed successfully**

**Key Achievements:**
- âœ… WebSocket connection pool implemented with bounded channels
- âœ… Graceful shutdown for 10K+ concurrent connections
- âœ… Comprehensive error handling and recovery
- âœ… 100% test coverage (47/47 tests passing)
- âœ… All security and quality reviews passed
- âœ… Changes committed to main workspace

**Learnings Captured:**
- 1 new reflection stored (async error handling)
- 1 new semantic pattern (bounded-channel-connection-pool)
- Knowledge graph updated with 5 entities, 4 relations

**Pipeline Efficiency:**
- Total duration: 8m 32s
- Model cost: $0.42
- TDD strikes: 2/3 (resolved on second attempt)
- Verification: First-pass after Strike 2 fix

**Next Steps:**
- Consider performance optimizations (Arc instead of clone)
- Add rate limiting to WebSocket endpoint (security suggestion)
- Extract validation helper function (simplicity suggestion)

---

## Infrastructure Cleanup

- âœ… Shadow container stopped and removed
- âœ… Redis Blackboard artifacts archived
- âœ… PostgreSQL memory consolidated
- âœ… Temporary files cleaned up

**Blackboard data retained for:** 7 days (then auto-purged)

---

**RVGB Pipeline ID:** rvgb-1732880400-a1b2c3
**Timestamp:** 2025-11-29T10:38:32Z
**Version:** RVGB v3.0
```

---

## SKIP OPTIONS

User can skip auto-steps by saying:

**Infrastructure:**
- "offline mode" â†’ Skip infrastructure checks, use JSON fallbacks
- "no docker" â†’ Skip shadow workspace, use git worktree instead

**Verification:**
- "skip tests" â†’ Skip STEP 10 (TDD Loop)
- "skip review" â†’ Skip STEP 11 (Multi-perspective review)
- "skip verification" â†’ Skip STEP 9 (Verification gauntlet)

**Memory:**
- "skip memory" â†’ Skip STEP 2 (Memory retrieval)
- "skip consolidation" â†’ Skip STEP 14 (Memory consolidation)
- "skip reflection" â†’ Skip STEP 12 (Reflexion)

**Quick Mode:**
- "quick mode" â†’ Skip: STEP 9, 10, 11, 12, 14 (verification, TDD, reviews, reflexion, consolidation)
- "direct mode" â†’ Skip: STEP 1, 7, 13 (infrastructure, shadow workspace, commit isolation)

**Debug Mode:**
- "keep shadow" â†’ Don't clean up shadow container (for debugging)
- "verbose" â†’ Include all Blackboard operations in report

---

## FALLBACK MODES

**If infrastructure unavailable:**

### PostgreSQL â†’ JSON Files
```
Memory location: /home/rnd/.claude/orchestrator/memory/
- episodic_memory.json (raw episodes)
- semantic_memory.json (patterns)
- procedural_memory.json (reflections)
- knowledge_graph.json (entities + relations)
```

### Redis â†’ In-Memory Dict
```python
# In-memory Blackboard (no persistence across runs)
blackboard = {
    "artifacts": {},
    "locks": {},
    "status": {},
    "agents": {}
}
```

### Docker â†’ Git Worktree
```bash
# Use git worktree instead of Docker container
git worktree add .worktrees/$PIPELINE_ID
# Work in .worktrees/$PIPELINE_ID/
# Merge or delete worktree at end
```

### MCP â†’ Direct Queries
```bash
# Instead of mcp__memory__search_memory()
psql $DATABASE_URL -c "SELECT * FROM episodic_memory WHERE ..."
```

---

## ORCHESTRATOR RESPONSIBILITIES

**DO:**
- âœ… Delegate ALL implementation to agents
- âœ… Coordinate via Blackboard
- âœ… Run verification in shadow workspace
- âœ… Update memory after completion
- âœ… Generate reflections from failures
- âœ… Provide comprehensive reports

**DO NOT:**
- âŒ Write implementation code yourself
- âŒ Skip verification steps (unless user says "skip")
- âŒ Forget to update memory/knowledge graph
- âŒ Commit without shadow verification
- âŒ Ignore critical review findings

---

## BEGIN RVGB ORCHESTRATION

Execute the 15-step RVGB pipeline for the task specified above.
